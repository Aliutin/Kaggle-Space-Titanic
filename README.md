# Space Titanic Survival Prediction

This repository contains code for predicting whether passengers on the Space Titanic were transported to another dimension or not. This project addresses a Kaggle competition, focusing on data cleaning, feature engineering, and the application of various machine learning models, with a particular emphasis on LightGBM and Random Forest.

## Project Overview

The goal of this project is to build a robust classification model to predict passenger outcomes on the Space Titanic. The dataset includes various passenger attributes such as age, spending habits, origin, destination, and cabin information.

## Features and Techniques

This project utilizes a comprehensive machine learning pipeline, including:

- **Data Loading and Initial Exploration:** Reading and understanding the structure of the provided `train.csv` and `test.csv` datasets.
- **Feature Engineering:**
    - Creation of `Total_bill` from individual expenditure columns.
    - Extraction of `Cabin_level`, `Cabin_number`, and `Cabin_type` from the `Cabin` string.
    - Derivation of `First_name` and `Last_name` from the `Name` column.
    - Imputation strategies for `CryoSleep`, `HomePlanet`, and `VIP` based on logical rules and other features.
    - Advanced feature creation like `Group_size`, `Luxury_spending_ratio`, `Food_spending_ratio`, `Age_group`, `Family_size`, `Solo_traveler`, and `Is_premium_deck`.
    - Application of `TF-IDF` and `TruncatedSVD` for text features derived from `First_name`, `Last_name`, and `Cabin`.
- **Missing Value Imputation:** Utilized K-Nearest Neighbors (KNN) based imputation for numeric and categorical columns where logical imputation was not sufficient.
- **Categorical Feature Encoding:** One-Hot Encoding (`OneHotEncoder`) for all categorical features.
- **Model Training and Evaluation:**
    - Comparison of multiple classification algorithms: Logistic Regression, K-Nearest Neighbors, Support Vector Classifier, Random Forest, XGBoost, LightGBM, CatBoost, and Gaussian Naive Bayes.
    - Hyperparameter tuning using `GridSearchCV` for initial model selection.
    - Advanced hyperparameter optimization using `Optuna` for Random Forest and LightGBM to find optimal model configurations.
    - Cross-validation (`RepeatedStratifiedKFold`) for robust model evaluation.
    - Visualization of model performance using Confusion Matrices, Feature Importance plots, and ROC Curves.

## Repository Structure

.
├── train.csv           # Training data
├── test.csv            # Test data
├── gender_submission.csv # Sample submission file
├── space_titanic_predictions_Forest.csv # Predictions generated by Random Forest model
├── space_titanic_predictions_LGBM.csv # Predictions generated by LightGBM model (final submission)
└── README.md           # This file


*(Note: The Python script provided in the prompt is intended to be run and its outputs (like the `predictions.csv` files) are part of the repository.)*

## Setup and Installation

To run this project locally, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/YOUR_USERNAME/Space-Titanic-Survival-Prediction.git](https://github.com/YOUR_USERNAME/Space-Titanic-Survival-Prediction.git)
    cd Space-Titanic-Survival-Prediction
    ```
    *(Replace `YOUR_USERNAME` with your actual GitHub username and the repository name if it's different)*

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    # On Windows
    .\venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install the required packages:**
    ```bash
    pip install numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm catboost optuna
    ```
    *(Ensure you have `tensorflow` or `pytorch` installed if you intend to use GPU acceleration for LightGBM, or remove `device='gpu'` and `gpu_use_dp=False` from the LightGBM objective function if you don't have a GPU or encounter issues).*

4.  **Download the dataset:**
    Download `train.csv`, `test.csv`, and `gender_submission.csv` from the [Kaggle Space Titanic competition page](https://www.kaggle.com/competitions/spaceship-titanic/data) and place them in the root directory of this project.

## Usage

The provided Python code (`your_script_name.py` - assuming you save the provided code as a Python file) executes the entire pipeline from data loading to prediction generation.

To run the analysis and generate predictions:

```bash
python your_script_name.py
This script will perform:

Data loading and initial cleaning.
Extensive feature engineering and imputation.
Model selection and hyperparameter tuning using GridSearchCV.
In-depth hyperparameter optimization for Random Forest and LightGBM using Optuna.
Training of the best-performing LightGBM model on the full training dataset.
Generation of space_titanic_predictions_Forest.csv and space_titanic_predictions_LGBM.csv files in the root directory, which can be submitted to the Kaggle competition.
Results
The LightGBM model, after extensive Optuna optimization, demonstrated strong performance. The space_titanic_predictions_LGBM.csv file contains the final predictions from this model.

(You might want to add specific accuracy scores here from your runs, e.g., "Achieved a test accuracy of X.XX% with the final LightGBM model." You can also include the best parameters found by Optuna.)

Best LightGBM Parameters (from Optuna)
Python

parameters = {
    'n_estimators': 621,
    'learning_rate': 0.012936722174568344,
    'num_leaves': 288,
    'max_depth': 6,
    'min_child_samples': 14,
    'lambda_l1': 0.007560925557891323,
    'lambda_l2': 0.012214066472269627,
    'feature_fraction': 0.5030338076535149,
    'bagging_fraction': 0.7498166252064917,
    'bagging_freq': 6
}
Visualizations
The script also generates several plots to help understand the model's performance and important features:

Confusion Matrix: Visualizes the model's true positive, true negative, false positive, and false negative predictions.
Feature Importance: Shows the relative importance of different features in the LightGBM model's predictions.
ROC Curves: Illustrates the trade-off between the true positive rate and false positive rate at various threshold settings.
Contributing
Feel free to fork this repository, open issues, or submit pull requests.